The 'proper architecture' question is certainly an interesting one. I've seen a 
lot of discussion recently on how a cardinal problem in machine learning today 
is that no one actually understands what's happening, and this leads to 
situations in which wildly different architectures can produce similar results 
with enough hyperparameter optimization, even though one architecture might seem 
superficially much better suited to a particular problem. This is an issue for 
enterprise and medical organizations, because they need to know how the 
conclusions of a network are reached, but I also see it as implying that the 
amount of computational power being supplied may be far greater than necessary 
in many cases. If two networks with very different internal data flow (e.g. 
convolutional layers vs simple all-to-all layers) can achieve nearly the same 
results on the same data, that suggests to me that both networks are arriving at 
states that contain in some way the (or an) 'optimal' structure for solving the 
problem.

At the moment, this is little more than a half-developed thought, but I think the principle that informs it is almost obvious: given an array of computational units with configurable internal parameters and connections (nodes and edges in a neural net) and an algorithm that optimizes the configuration to move towards good results (gradient descent), it's probably always possible to brute-force a solution to a problem as long as the algorithm has enough nodes to work with.

However, simply by having some information about the input data, we can optimize the general structure of the network to better fit the data, e.g. convolutional layers for image processing. But when you put together a massive number of specialized layers together to create a monster of a network, such as is usually seen in releases from Google and the like, to me that looks like a retreat in the direction of throwing a lot of computational power into the hands of an optimization algorithm and starting to pray, and I think this is reflected by the emerging sense that hyperparameters; i.e., the specific values controlling the optimizer, have an outsize impact on network performance.

What we're doing moves distinctly in the opposite direction. Our architectures are designed from the ground up with both the structure of the input data and, probably more crucially, the structure of the thing creating that data in mind. Additionally, our computational footprint is orders of magnitude smaller than that of most networks featured in major developments in ML. Basically, I think that we're 
